<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>ðŸ¤– AI'Han Academy - Computer Vision Lab (Optimized)</title>
    <!-- Bootstrap 5.3 CSS -->
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <!-- Font Awesome -->
    <link
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css"
      rel="stylesheet"
    />
    <!-- TensorFlow.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/4.0.0/tf.min.js"></script>
    <!-- TensorFlow Pose Detection -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection@2.1.0/dist/pose-detection.min.js"></script>
    <!-- MediaPipe Hands -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <!-- MediaPipe Camera Utils -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <!-- MediaPipe Face Mesh -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <!-- Face API for Real-Time Emotion Analysis -->
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <!-- Chart.js for Real-Time Emotion Graphs -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <!-- Canvas-Confetti for Celebration Effects -->
    <script src="https://cdn.jsdelivr.net/npm/canvas-confetti@1.5.1/dist/confetti.browser.min.js"></script>
    <!-- TF COCO-SSD for Object Detection -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

    <style>
      :root {
        --primary-color: #3498db;
        --secondary-color: #2ecc71;
        --dark-bg: #1e272e;
        --light-bg: #f5f5f5;
      }
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }
      body {
        font-family: 'IBM Plex Sans', Arial, sans-serif;
        background: linear-gradient(135deg, var(--light-bg) 0%, #e0e0e0 100%);
        color: var(--dark-bg);
        line-height: 1.6;
      }
      .lang-toggle {
        position: absolute;
        top: 10px;
        left: 10px;
        z-index: 1000;
      }
      .lang-toggle button {
        background-color: var(--secondary-color);
        border: none;
        border-radius: 5px;
        padding: 5px 10px;
        margin-right: 5px;
        color: white;
        cursor: pointer;
      }
      .header {
        background: linear-gradient(to right, var(--primary-color), var(--secondary-color));
        color: white;
        text-align: center;
        padding: 20px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        position: relative;
      }
      .header h1 {
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: 600;
        flex-wrap: wrap;
        gap: 10px;
      }
      .header h1 i {
        font-size: 1.8em;
      }
      .speech-bubble {
        display: inline-block;
        background: #ffdd57;
        color: #333;
        padding: 5px 10px;
        border-radius: 10px;
        position: relative;
        animation: bubbleMove 2s infinite ease-in-out;
      }
      @keyframes bubbleMove {
        0% {
          transform: translateY(0);
          background-color: #ffdd57;
        }
        50% {
          transform: translateY(-5px);
          background-color: #ffe680;
        }
        100% {
          transform: translateY(0);
          background-color: #ffdd57;
        }
      }
      .card {
        border: none;
        border-radius: 12px;
        overflow: hidden;
        box-shadow: 0 10px 25px rgba(0,0,0,0.1);
        transition: transform 0.3s ease;
        background: white;
        margin-bottom: 20px;
      }
      .card:hover {
        transform: scale(1.02);
      }
      .card-header {
        background: linear-gradient(to right, var(--primary-color), var(--secondary-color));
        color: white;
        font-size: 1.25rem;
      }
      canvas, video {
        width: 100%;
        border-radius: 8px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05);
      }
      .controls .btn {
        padding: 10px 20px;
        border: none;
        border-radius: 5px;
        cursor: pointer;
        transition: all 0.3s ease;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 10px;
      }
      .btn-start {
        background-color: var(--primary-color);
        color: white;
      }
      .btn-stop {
        background-color: #e74c3c;
        color: white;
      }
      .share-btn {
        background-color: var(--secondary-color);
        color: white;
        border: none;
        border-radius: 5px;
        padding: 8px 15px;
        margin-top: 10px;
        cursor: pointer;
      }
      .stats {
        display: flex;
        justify-content: space-between;
        margin-top: 10px;
        font-size: 0.9em;
        color: #7f8c8d;
      }
      .footer {
        background: var(--dark-bg);
        color: white;
        text-align: center;
        padding: 15px;
        margin-top: 20px;
      }
      .footer a {
        color: var(--secondary-color);
        margin: 0 10px;
        text-decoration: none;
      }
      .spinner {
        border: 4px solid rgba(0,0,0,0.1);
        width: 36px;
        height: 36px;
        border-radius: 50%;
        border-left-color: var(--primary-color);
        animation: spin 1s ease infinite;
        margin: auto;
        display: none;
      }
      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }
      .task-message {
        font-weight: bold;
        color: var(--primary-color);
        text-align: center;
        margin-top: 5px;
      }
      .info-panel {
        background: #ffffff;
        border: 2px solid var(--primary-color);
        border-radius: 10px;
        padding: 20px;
        margin-bottom: 20px;
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
      }
      .info-panel h2 {
        color: var(--primary-color);
      }
      .faq-panel {
        background: #f8f9fa;
        border: 1px solid #ced4da;
        border-radius: 10px;
        padding: 15px;
        margin-top: 20px;
      }
      .faq-panel h3 {
        color: var(--primary-color);
      }
      .edu-module pre {
        background: #f0f0f0;
        padding: 10px;
        border-radius: 5px;
        overflow-x: auto;
      }
      .edu-module .output-box {
        background: #eaeaea;
        padding: 10px;
        border-radius: 5px;
        margin-top: 10px;
        font-family: monospace;
      }
    </style>
  </head>
  <body>
    <!-- Language Toggle Buttons (EN / TR) -->
    <div class="lang-toggle">
      <button id="btnEN">EN</button>
      <button id="btnTR">TR</button>
    </div>

    <header class="header">
      <h1>
        <i class="fas fa-robot"></i>
        <span class="speech-bubble" data-translate="greeting">Hello, I'm AI'Han!</span>
        <span data-translate="portalTitle">AI'Han Academy - Computer Vision Lab</span>
      </h1>
    </header>

    <!-- Top Info Section -->
    <div class="container my-4">
      <div class="info-panel" data-translate="platformInfo">
        <h2 data-translate="howItWorksTitle">How Our Platform Works</h2>
        <!-- Extended descriptive text for the platform -->
        <p data-translate="platformDesc">
          Our lab leverages cutting-edge AI and machine learning technologies to integrate state-of-the-art modules including Body Pose Analysis, Hand Gesture Detection, Emotion Analysis, Face Recognition & Speech Estimation, Object Detection, Motion Detection, Fun Icon Overlay, and Emoji Party. Each module is built upon advanced algorithms that push the boundaries of computer vision.
        </p>
        <p data-translate="bodyPoseInfo"><strong>Body Pose Analysis:</strong> Powered by TensorFlow.js and MoveNet, this module detects and tracks body keypoints in real time.</p>
        <p data-translate="handGestureInfo"><strong>Hand Gesture Detection:</strong> Utilizing MediaPipe Hands to accurately interpret your hand movements.</p>
        <p data-translate="emotionInfo"><strong>Emotion Analysis:</strong> With face-api.js, it analyzes facial expressions to determine your mood.</p>
        <p data-translate="faceRecogInfo"><strong>Face Recognition & Speech Estimation:</strong> This module uses MediaPipe Face Mesh to capture 468 facial landmarks and simulate speech estimation.</p>
        <p data-translate="platformConclusion">
          Explore our platform to experience the transformative power of computer vision in real time.
        </p>
      </div>

      <!-- Accordion: Additional Info -->
      <div class="accordion" id="cvAccordion">
        <!-- What is Computer Vision? -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="headingOne">
            <button
              class="accordion-button"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#collapseOne"
              aria-expanded="true"
              aria-controls="collapseOne"
              data-translate="cvWhatTitle"
            >
              What is Computer Vision?
            </button>
          </h2>
          <div
            id="collapseOne"
            class="accordion-collapse collapse show"
            aria-labelledby="headingOne"
            data-bs-parent="#cvAccordion"
          >
            <div class="accordion-body" data-translate="cvWhatContent">
              <p>
                Computer Vision is a branch of artificial intelligence that enables computers to interpret and understand digital images and videos. It combines techniques from image processing, machine learning, and deep learning to identify objects, classify scenes, and perform a wide range of visual tasks.
              </p>
            </div>
          </div>
        </div>
        <!-- Which AI Domain Does It Belong To? -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="headingTwo">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#collapseTwo"
              aria-expanded="false"
              aria-controls="collapseTwo"
              data-translate="cvAITitle"
            >
              Which AI Domain Does It Belong To?
            </button>
          </h2>
          <div
            id="collapseTwo"
            class="accordion-collapse collapse"
            aria-labelledby="headingTwo"
            data-bs-parent="#cvAccordion"
          >
            <div class="accordion-body" data-translate="cvAIContent">
              <p>
                Computer Vision is a vital subfield of artificial intelligence that lies at the intersection of machine learning and deep learning. It focuses on enabling machines to extract meaningful information from visual data, making it integral to robotics, autonomous systems, and real-time decision-making applications.
              </p>
            </div>
          </div>
        </div>
        <!-- History and Evolution -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="headingThree">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#collapseThree"
              aria-expanded="false"
              aria-controls="collapseThree"
              data-translate="cvHistoryTitle"
            >
              History and Evolution
            </button>
          </h2>
          <div
            id="collapseThree"
            class="accordion-collapse collapse"
            aria-labelledby="headingThree"
            data-bs-parent="#cvAccordion"
          >
            <div class="accordion-body" data-translate="cvHistoryContent">
              <p>
                The field of Computer Vision began in the 1960s with basic image processing techniques. With the rise of computational power and deep learning, it has evolved dramatically. BilgisayarlÄ± gÃ¶rÃ¼, 1960â€™larda temel gÃ¶rÃ¼ntÃ¼ iÅŸleme teknikleriyle baÅŸlamÄ±ÅŸ, sonrasÄ±nda hesaplama gÃ¼cÃ¼ndeki artÄ±ÅŸ ve derin Ã¶ÄŸrenme ile devrim niteliÄŸinde geliÅŸmeler yaÅŸamÄ±ÅŸtÄ±r. It now encompasses advanced tasks such as object recognition and scene understanding.
              </p>
            </div>
          </div>
        </div>
        <!-- Python Code Examples -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="headingFour">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#collapseFour"
              aria-expanded="false"
              aria-controls="collapseFour"
              data-translate="cvPythonTitle"
            >
              Python Code Examples
            </button>
          </h2>
          <div
            id="collapseFour"
            class="accordion-collapse collapse"
            aria-labelledby="headingFour"
            data-bs-parent="#cvAccordion"
          >
            <div class="accordion-body" data-translate="cvPythonContent">
              <p>
                Below are detailed Python examples demonstrating key computer vision techniques.
              </p>
              <pre><code class="python">
# Example: Face Detection using OpenCVâ€™s Haar Cascades
import cv2
import numpy as np

# Load the cascade classifier for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Read the image
img = cv2.imread('test.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Detect faces in the image
faces = face_cascade.detectMultiScale(gray, 1.1, 4)

# Draw rectangles around detected faces
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)

# Display the output
cv2.imshow('Face Detection', img)
cv2.waitKey(0)
cv2.destroyAllWindows()
              </code></pre>
            </div>
          </div>
        </div>
        <!-- Sources -->
        <div class="accordion-item">
          <h2 class="accordion-header" id="headingFive">
            <button
              class="accordion-button collapsed"
              type="button"
              data-bs-toggle="collapse"
              data-bs-target="#collapseFive"
              aria-expanded="false"
              aria-controls="collapseFive"
              data-translate="cvSourcesTitle"
            >
              Sources
            </button>
          </h2>
          <div
            id="collapseFive"
            class="accordion-collapse collapse"
            aria-labelledby="headingFive"
            data-bs-parent="#cvAccordion"
          >
            <div class="accordion-body" data-translate="cvSourcesContent">
              <ul>
                <li>
                  <a href="https://www.coursera.org/specializations/computer-vision" target="_blank">
                    Coursera - Computer Vision Specialization
                  </a>: A comprehensive program covering advanced computer vision techniques.
                </li>
                <li>
                  <a href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" target="_blank">
                    Udacity - Computer Vision Nanodegree
                  </a>: Hands-on projects and real-world applications in computer vision.
                </li>
                <li>
                  <a href="https://github.com/opencv/opencv" target="_blank">
                    OpenCV GitHub
                  </a>: The open source repository for the widely-used computer vision library.
                </li>
                <li>
                  <a href="https://www.tensorflow.org/" target="_blank">
                    TensorFlow
                  </a>: A robust platform for developing deep learning models.
                </li>
                <li>
                  <a href="https://mediapipe.dev/" target="_blank">
                    MediaPipe
                  </a>: An innovative framework for real-time computer vision solutions.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!-- End Top Info Section -->

    <br />

    <!-- Main Modules Section -->
    <div class="container my-4">
      <div class="row g-4">
        <!-- (C) Body Pose Analysis Module -->
        <div class="col-md-6">
          <div class="card">
            <div class="card-header text-center" data-translate="bodyPoseTitle">
              <i class="fas fa-person"></i>
              <span data-translate="bodyPoseTitle">Body Pose Analysis</span>
            </div>
            <div class="card-body">
              <!-- Educational Module -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduBodyPose" aria-expanded="false" aria-controls="eduBodyPose">
                  Educational Module
                </button>
                <div class="collapse" id="eduBodyPose">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Body Pose Analysis using TensorFlow and MoveNet
import tensorflow as tf
import cv2
import numpy as np

model = tf.saved_model.load('movenet_model_directory')
frame = cv2.imread('person.jpg')
results = model(frame)
print("Pose detected: Right arm raised!")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Pose detected: Right arm raised!</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="bodyLoader" class="spinner"></div>
              <video id="bodyVideo" style="display: none;"></video>
              <canvas id="bodyCanvas"></canvas>
              <div id="bodyTask" class="task-message" data-translate="bodyTaskMsg">Task: Raise your right arm!</div>
              <div class="stats mt-2">
                <span id="bodyFps">FPS: -</span>
                <span id="bodyLandmarks">Landmarks: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startBodyPoseDetection()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopBodyPoseDetection()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Body Pose Analysis')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#bodyInfo"
                  aria-expanded="false"
                  aria-controls="bodyInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="bodyInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="bodyHowItWorksTitle">Body Pose Analysis</strong><br />
                    <span data-translate="bodyHowItWorksTextEN" style="display: none;">
                      This module uses TensorFlow.js and the MoveNet model to accurately detect and track body keypoints in real time.
                    </span>
                    <span data-translate="bodyHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, TensorFlow.js ve MoveNet modeli kullanarak vÃ¼cudunuzun ana noktalarÄ±nÄ± gerÃ§ek zamanlÄ± olarak algÄ±lar ve izler.
                    </span>
                    <div id="bodyHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- (D) Hand Gesture Detection Module -->
        <div class="col-md-6">
          <div class="card">
            <div class="card-header text-center" data-translate="handTitle">
              <i class="fas fa-hand-paper"></i>
              <span data-translate="handTitle">Hand Gesture Detection</span>
            </div>
            <div class="card-body">
              <!-- Educational Module -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduHandGesture" aria-expanded="false" aria-controls="eduHandGesture">
                  Educational Module
                </button>
                <div class="collapse" id="eduHandGesture">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Hand Gesture Detection using MediaPipe
import mediapipe as mp
import cv2

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1)
frame = cv2.imread('hand.jpg')
results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
if results.multi_hand_landmarks:
    print("Hand gesture detected: Waving!")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Hand gesture detected: Waving!</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="handLoader" class="spinner"></div>
              <video id="handVideo" style="display: none;"></video>
              <canvas id="handCanvas"></canvas>
              <div id="handTask" class="task-message" data-translate="handTaskMsg">Task: Wave your hand!</div>
              <div class="stats mt-2">
                <span id="handFps">FPS: -</span>
                <span id="handLandmarks">Landmarks: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startHandPoseDetection()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopHandPoseDetection()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Hand Gesture Detection')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#handInfo"
                  aria-expanded="false"
                  aria-controls="handInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="handInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="handHowItWorksTitle">Hand Gesture Detection</strong><br />
                    <span data-translate="handHowItWorksTextEN" style="display: none;">
                      This module uses MediaPipe Hands to analyze your finger positions and recognize specific gestures.
                    </span>
                    <span data-translate="handHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, MediaPipe Hands kullanarak parmak pozisyonlarÄ±nÄ±zÄ± analiz eder ve belirli hareketleri algÄ±lar.
                    </span>
                    <div id="handHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- (E) Emotion Analysis Module -->
        <div class="col-12">
          <div class="card">
            <div class="card-header text-center" data-translate="emotionTitle">
              <i class="fas fa-smile"></i>
              <span data-translate="emotionTitle">Emotion Analysis</span>
            </div>
            <div class="card-body">
              <!-- Educational Module -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduEmotion" aria-expanded="false" aria-controls="eduEmotion">
                  Educational Module
                </button>
                <div class="collapse" id="eduEmotion">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Emotion Analysis using OpenCV and a pre-trained model
import cv2
import numpy as np

frame = cv2.imread('face.jpg')
print("Detected emotion: Happy (85%)")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Detected emotion: Happy (85%)</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="emotionLoader" class="spinner"></div>
              <video id="emotionVideo" style="display: none;"></video>
              <canvas id="emotionCanvas"></canvas>
              <div id="emotionTask" class="task-message" data-translate="emotionTaskMsg">Task: Show your facial expression!</div>
              <div class="stats mt-2">
                <span id="emotionFps">FPS: -</span>
                <span id="emotionResult">Emotion: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startEmotionAnalysis()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopEmotionAnalysis()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Emotion Analysis')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#emotionInfo"
                  aria-expanded="false"
                  aria-controls="emotionInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="emotionInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="emotionHowItWorksTitle">Emotion Analysis</strong><br />
                    <span data-translate="emotionHowItWorksTextEN" style="display: none;">
                      This module uses face-api.js along with deep learning models to analyze your facial expressions in real time.
                    </span>
                    <span data-translate="emotionHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, face-api.js ve derin Ã¶ÄŸrenme modellerini kullanarak yÃ¼z ifadenizi gerÃ§ek zamanlÄ± olarak analiz eder.
                    </span>
                    <div id="emotionHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
              <div class="mt-4">
                <h6 class="text-center" data-translate="emotionChartTitle">Emotion Distribution Graph</h6>
                <canvas id="emotionChart"></canvas>
              </div>
            </div>
          </div>
        </div>

        <!-- (F) Face Recognition & Speech Estimation Module -->
        <div class="col-12">
          <div class="card">
            <div class="card-header text-center" data-translate="faceRecogTitle">
              <i class="fas fa-user"></i>
              <span data-translate="faceRecogTitle">Face Recognition &amp; Speech Estimation</span>
            </div>
            <div class="card-body">
              <!-- Educational Module -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduFaceRecog" aria-expanded="false" aria-controls="eduFaceRecog">
                  Educational Module
                </button>
                <div class="collapse" id="eduFaceRecog">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Face Recognition using OpenCV and dlib
import cv2
import dlib

detector = dlib.get_frontal_face_detector()
img = cv2.imread('face.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
faces = detector(gray)
if faces:
    print("Face recognized: John Doe")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Face recognized: John Doe</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="faceLoader" class="spinner"></div>
              <video id="faceVideo" style="display: none;"></video>
              <!-- Enlarged canvas for better face details -->
              <canvas id="faceCanvas" style="height: 500px; width: 100%; object-fit: cover;"></canvas>
              <div id="faceSpeech" class="task-message" data-translate="faceSpeechMsg">
                Speech Estimation: (Simulation: Speaking...)
              </div>
              <div class="stats mt-2">
                <span id="faceFps">FPS: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startFaceDetection()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopFaceDetection()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Face Recognition & Speech Estimation')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#faceInfo"
                  aria-expanded="false"
                  aria-controls="faceInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="faceInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="faceHowItWorksTitle">Face Recognition (Face Mesh) &amp; Speech Estimation</strong><br />
                    <span data-translate="faceHowItWorksTextEN" style="display: none;">
                      This module uses MediaPipe Face Mesh to detect 468 facial landmarks and simulates speech estimation based on facial movements.
                    </span>
                    <span data-translate="faceHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, MediaPipe Face Mesh kullanarak 468 yÃ¼z iÅŸaret noktasÄ±nÄ± tespit eder ve yÃ¼z hareketlerine dayalÄ± konuÅŸma tahmini simÃ¼lasyonu gerÃ§ekleÅŸtirir.
                    </span>
                    <div id="faceHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- Additional Example Projects Section -->
      <div class="row g-4 mt-4">
        <!-- (G) Object Detection (COCO-SSD) -->
        <div class="col-md-6">
          <div class="card">
            <div class="card-header text-center" data-translate="objectTitle">
              <i class="fas fa-shapes"></i>
              <span data-translate="objectTitle">Object Detection (COCO-SSD)</span>
            </div>
            <div class="card-body">
              <!-- Educational Module -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduObject" aria-expanded="false" aria-controls="eduObject">
                  Educational Module
                </button>
                <div class="collapse" id="eduObject">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Object Detection using TensorFlow and COCO-SSD
import cv2
import numpy as np
import tensorflow as tf

print("Object detected: Person (95% confidence)")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Object detected: Person (95% confidence)</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="objectLoader" class="spinner"></div>
              <video id="objectVideo" style="display: none;"></video>
              <canvas id="objectCanvas"></canvas>
              <div id="objectTask" class="task-message" data-translate="objectTaskMsg">
                Task: Show an object in front of your camera!
              </div>
              <div class="stats mt-2">
                <span id="objectFps">FPS: -</span>
                <span id="objectDetections">Detections: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startObjectDetection()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopObjectDetection()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Object Detection (COCO-SSD)')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#objectInfo"
                  aria-expanded="false"
                  aria-controls="objectInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="objectInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="objectHowItWorksTitle">Object Detection (COCO-SSD)</strong><br />
                    <span data-translate="objectHowItWorksTextEN" style="display: none;">
                      This module utilizes the COCO-SSD model to detect various objects in real time.
                    </span>
                    <span data-translate="objectHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, COCO-SSD modelini kullanarak gerÃ§ek zamanlÄ± olarak Ã§eÅŸitli nesneleri tespit eder.
                    </span>
                    <div id="objectHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- (H) Motion Detection (Background Subtraction) -->
        <div class="col-md-6">
          <div class="card">
            <div class="card-header text-center" data-translate="motionTitle">
              <i class="fas fa-user-secret"></i>
              <span data-translate="motionTitle">Motion Detection (Background Subtraction)</span>
            </div>
            <div class="card-body">
              <!-- Educational Module -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduMotion" aria-expanded="false" aria-controls="eduMotion">
                  Educational Module
                </button>
                <div class="collapse" id="eduMotion">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Motion Detection using background subtraction
import cv2

print("Motion detected!")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Motion detected!</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="motionLoader" class="spinner"></div>
              <video id="motionVideo" style="display: none;"></video>
              <canvas id="motionCanvas"></canvas>
              <div id="motionTask" class="task-message" data-translate="motionTaskMsg">
                Task: Move or wave something to trigger motion detection!
              </div>
              <div class="stats mt-2">
                <span id="motionFps">FPS: -</span>
                <span id="motionStatus">Motion: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startMotionDetection()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopMotionDetection()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Motion Detection (Background Subtraction)')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#motionInfo"
                  aria-expanded="false"
                  aria-controls="motionInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="motionInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="motionHowItWorksTitle">Motion Detection (Background Subtraction)</strong><br />
                    <span data-translate="motionHowItWorksTextEN" style="display: none;">
                      This module implements a simple background subtraction algorithm to detect motion.
                    </span>
                    <span data-translate="motionHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, arka plan Ã§Ä±karma algoritmasÄ± kullanarak hareketi tespit eder.
                    </span>
                    <div id="motionHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- NEW: Additional Modules Row for Fun Icon Overlay & Emoji Party -->
      <div class="row g-4 mt-4">
        <!-- (I) Fun Icon Overlay Module -->
        <div class="col-md-6">
          <div class="card">
            <div class="card-header text-center" data-translate="iconOverlayTitle">
              <i class="fas fa-smile-beam"></i>
              <span data-translate="iconOverlayTitle">Fun Icon Overlay</span>
            </div>
            <div class="card-body">
              <!-- Educational Module for Fun Icon Overlay -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduIconOverlay" aria-expanded="false" aria-controls="eduIconOverlay">
                  Educational Module
                </button>
                <div class="collapse" id="eduIconOverlay">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Simulating Fun Icon Overlay
print("Overlaying fun icons with oscillating effect...")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Overlaying fun icons with oscillating effect...</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="iconOverlayLoader" class="spinner"></div>
              <video id="iconOverlayVideo" style="display: none;"></video>
              <canvas id="iconOverlayCanvas"></canvas>
              <div id="iconOverlayTask" class="task-message" data-translate="iconOverlayTaskMsg">
                Task: Enjoy the fun icons overlay!
              </div>
              <div class="stats mt-2">
                <span id="iconOverlayFps">FPS: -</span>
                <span id="iconOverlayLevel">Level: 1</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startIconOverlay()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopIconOverlay()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Fun Icon Overlay')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#iconOverlayInfo"
                  aria-expanded="false"
                  aria-controls="iconOverlayInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="iconOverlayInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="iconOverlayHowItWorksTitle">Fun Icon Overlay</strong><br />
                    <span data-translate="iconOverlayHowItWorksTextEN" style="display: none;">
                      This module uses your camera feed to overlay fun icons that add a playful twist. A level system increases your excitement by shortening the icon update interval as you level up.
                    </span>
                    <span data-translate="iconOverlayHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, kameranÄ±zÄ±n gÃ¶rÃ¼ntÃ¼sÃ¼ne eÄŸlenceli ikonlar ekler. Entegre seviye sistemi sayesinde, seviye yÃ¼kseldikÃ§e ikon gÃ¼ncelleme aralÄ±ÄŸÄ± kÄ±salÄ±r ve deneyiminiz daha dinamik hale gelir.
                    </span>
                    <div id="iconOverlayHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <!-- (J) Emoji Party Module -->
        <div class="col-md-6">
          <div class="card">
            <div class="card-header text-center" data-translate="emojiPartyTitle">
              <i class="fas fa-grin-stars"></i>
              <span data-translate="emojiPartyTitle">Emoji Party</span>
            </div>
            <div class="card-body">
              <!-- Educational Module for Emoji Party -->
              <div class="mt-3 edu-module">
                <button class="btn btn-outline-secondary btn-sm" data-bs-toggle="collapse" data-bs-target="#eduEmojiParty" aria-expanded="false" aria-controls="eduEmojiParty">
                  Educational Module
                </button>
                <div class="collapse" id="eduEmojiParty">
                  <div class="card card-body mt-2">
                    <pre><code class="python">
# Example Python code for Interactive Emoji Party
print("Starting interactive emoji tasks...")
print("Task: Turn your head right -> Emoji: ðŸ˜Ž")
                    </code></pre>
                    <div class="output-box">
                      Output:
                      <pre>Starting interactive emoji tasks...
Task: Turn your head right -> Emoji: ðŸ˜Ž</pre>
                    </div>
                  </div>
                </div>
              </div>
              <div id="emojiPartyLoader" class="spinner"></div>
              <video id="emojiPartyVideo" style="display: none;"></video>
              <canvas id="emojiPartyCanvas"></canvas>
              <div id="emojiPartyTask" class="task-message" data-translate="emojiPartyTaskMsg">
                Task: Let the emoji party begin!
              </div>
              <div class="stats mt-2">
                <span id="emojiPartyFps">FPS: -</span>
                <span id="emojiPartyCount">Emojis: -</span>
              </div>
              <div class="controls mt-2 d-flex justify-content-center gap-2">
                <button class="btn btn-start" onclick="startEmojiParty()" data-translate="startBtn">Start</button>
                <button class="btn btn-stop" onclick="stopEmojiParty()" data-translate="stopBtn">Stop</button>
                <button class="share-btn" onclick="openShareModal('Emoji Party')" data-translate="shareBtn">Share!</button>
              </div>
              <div class="mt-3">
                <button
                  class="btn btn-outline-info btn-sm"
                  data-bs-toggle="collapse"
                  data-bs-target="#emojiPartyInfo"
                  aria-expanded="false"
                  aria-controls="emojiPartyInfo"
                  data-translate="howItWorksBtn"
                >
                  How it Works?
                </button>
                <div class="collapse" id="emojiPartyInfo">
                  <div class="card card-body mt-2">
                    <strong data-translate="emojiPartyHowItWorksTitle">Emoji Party</strong><br />
                    <span data-translate="emojiPartyHowItWorksTextEN" style="display: none;">
                      This module transforms your video feed into an interactive emoji party by using Face Mesh and Hands detection to trigger tasks such as turning your head, blinking, or placing your hand on your head. When a task is recognized, the corresponding emoji is displayed for 2 seconds before moving on.
                    </span>
                    <span data-translate="emojiPartyHowItWorksTextTR" style="display: none;">
                      Bu modÃ¼l, Face Mesh ve Hands tespiti ile baÅŸÄ±nÄ±zÄ± Ã§evirme, gÃ¶z kÄ±rpma veya elinizi baÅŸÄ±nÄ±za koyma gibi gÃ¶revleri algÄ±lar and once the task is completed, displays the corresponding emoji for 2 seconds before moving on.
                    </span>
                    <div id="emojiPartyHowItWorksDynamic"></div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <!-- End Additional Modules Row -->
    </div>
    <!-- End Main Modules Section -->

    <footer class="footer">
      <p>
        Â© 2024 AI'Han Academy All Rights Reserved |
        <a href="mailto:ayhanbzkrt@gmail.com">Email</a> |
        <a href="https://github.com/ayhanbzkrt" target="_blank">GitHub</a> |
        <a href="https://www.linkedin.com/in/ayhanbozkurt/" target="_blank">LinkedIn</a>
      </p>
      <p style="font-size: 0.8em; margin-top: 10px;">
        <strong>Data Security and Privacy:</strong> This application processes user data...
      </p>
      <p style="font-size: 0.8em; margin-top: 10px;">
        Use the "Share!" buttons to share your achievements on social media.
      </p>
    </footer>

    <!-- Help / FAQ Modal -->
    <div class="modal fade" id="helpModal" tabindex="-1" aria-labelledby="helpModalLabel" aria-hidden="true">
      <div class="modal-dialog modal-lg">
        <div class="modal-content">
          <div class="modal-header">
            <h5 class="modal-title" id="helpModalLabel" data-translate="helpModalTitle">Help &amp; FAQ</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>
          <div class="modal-body">
            <div class="faq-panel">
              <h3 data-translate="faqDataSecurity">Data Security and Privacy</h3>
              <p data-translate="faqDataSecurityDesc">
                This application runs solely in your browser and does not store any personal data on remote servers.
              </p>
            </div>
            <div class="faq-panel">
              <h3 data-translate="faqBodyPose">How does Body Pose Analysis work?</h3>
              <p data-translate="faqBodyPoseDesc">
                Using TensorFlow.js and MoveNet, the module detects key body points and analyzes your pose in real time.
              </p>
            </div>
            <div class="faq-panel">
              <h3 data-translate="faqHandGesture">How does Hand Gesture Detection work?</h3>
              <p data-translate="faqHandGestureDesc">
                MediaPipe Hands analyzes the position and movement of your fingers to recognize hand gestures.
              </p>
            </div>
            <div class="faq-panel">
              <h3 data-translate="faqEmotion">How does Emotion Analysis work?</h3>
              <p data-translate="faqEmotionDesc">
                Face-api.js along with deep learning models analyze your facial expression in real time to determine your mood.
              </p>
            </div>
            <div class="faq-panel">
              <h3 data-translate="faqFaceRecog">How does Face Recognition &amp; Speech Estimation work?</h3>
              <p data-translate="faqFaceRecogDesc">
                MediaPipe Face Mesh detects 468 facial landmarks which are used to perform face recognition and simulate speech estimation.
              </p>
            </div>
            <div class="faq-panel">
              <h3 data-translate="faqPerformance">Performance and Model Loading</h3>
              <p data-translate="faqPerformanceDesc">
                All models are loaded asynchronously in the background using WebGL acceleration to ensure optimal performance.
              </p>
            </div>
          </div>
          <div class="modal-footer">
            <button type="button" class="btn btn-secondary" data-bs-dismiss="modal" data-translate="faqCloseBtn">Close</button>
          </div>
        </div>
      </div>
    </div>

    <!-- Trigger Help / FAQ Modal -->
    <div style="text-align: center; margin: 20px 0;">
      <button class="btn btn-outline-primary" data-bs-toggle="modal" data-bs-target="#helpModal" data-translate="faqTriggerBtn">Help / FAQ</button>
    </div>

    <!-- Modal for Social Sharing -->
    <div class="modal fade" id="shareModal" tabindex="-1" aria-hidden="true">
      <div class="modal-dialog modal-sm modal-dialog-centered">
        <div class="modal-content">
          <div class="modal-header">
            <h5 class="modal-title">Share Your Achievement!</h5>
            <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
          </div>
          <div class="modal-body d-flex flex-column align-items-center">
            <p style="font-size: 0.9em; text-align: center;">
              AI'Han Academy Rocks! I'm diving deep into AI and computer visionâ€”come and experience it for yourself!
            </p>
            <div class="d-flex flex-column gap-2" style="width:100%;">
              <button class="btn btn-sm btn-outline-primary" id="shareTwitterBtn"><i class="fab fa-twitter"></i> Share on Twitter (X)</button>
              <button class="btn btn-sm btn-outline-primary" id="shareFacebookBtn"><i class="fab fa-facebook"></i> Share on Facebook</button>
              <button class="btn btn-sm btn-outline-primary" id="shareInstaBtn"><i class="fab fa-instagram"></i> Share on Instagram</button>
              <button class="btn btn-sm btn-outline-primary" id="shareLinkedinBtn"><i class="fab fa-linkedin"></i> Share on LinkedIn</button>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Bootstrap 5.3 JS Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Audio Elements for Sound Effects -->
    <audio id="bgMusic" src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3" loop></audio>
    <audio id="clickSound" src="https://www.soundjay.com/buttons/button-09.mp3"></audio>

    <script>
      tf.setBackend("webgl");

      /* ----------------------------------------------------------------
         (A) INTRO SEQUENCE (FACE DETECTION + TTS)
      ---------------------------------------------------------------- */
      let faceMeshIntro = null;
      let faceIntroCamera = null;
      let faceIntroStream = null;
      let introTimeout = null;
      const introTTS =
        "Welcome to AI'Han Academy - Computer Vision Lab! Get ready to explore the incredible world of AI!";

      async function startIntroFaceDetection() {
        faceMeshIntro = new FaceMesh({
          locateFile: (file) =>
            `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
        });
        faceMeshIntro.setOptions({
          maxNumFaces: 1,
          refineLandmarks: true,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5,
        });
        faceMeshIntro.onResults((results) => {
          const canvas = document.getElementById("faceCanvas");
          const ctx = canvas.getContext("2d");
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          if (results.multiFaceLandmarks && results.multiFaceLandmarks.length) {
            const landmarks = results.multiFaceLandmarks[0];
            landmarks.forEach((pt) => {
              ctx.beginPath();
              ctx.arc(pt.x * canvas.width, pt.y * canvas.height, 2, 0, 2 * Math.PI);
              ctx.fillStyle = "orange";
              ctx.fill();
            });
          }
        });

        try {
          faceIntroStream = await navigator.mediaDevices.getUserMedia({ video: true });
          const faceVideo = document.getElementById("faceVideo");
          faceVideo.srcObject = faceIntroStream;
          await faceVideo.play();
          const faceCanvas = document.getElementById("faceCanvas");
          faceCanvas.width = faceVideo.videoWidth;
          faceCanvas.height = faceVideo.videoHeight;

          faceIntroCamera = new Camera(faceVideo, {
            onFrame: async () => {
              await faceMeshIntro.send({ image: faceVideo });
            },
            width: faceVideo.videoWidth,
            height: faceVideo.videoHeight,
          });
          faceIntroCamera.start();
        } catch (err) {
          console.error("Intro face detection error:", err);
        }
      }

      function playIntroTTS() {
        const utter = new SpeechSynthesisUtterance(introTTS);
        utter.lang = "en-US";
        speechSynthesis.speak(utter);
      }

      function stopIntroFaceDetection() {
        if (faceIntroCamera) {
          faceIntroCamera.stop();
          faceIntroCamera = null;
        }
        if (faceIntroStream) {
          faceIntroStream.getTracks().forEach((t) => t.stop());
          faceIntroStream = null;
        }
      }

      function doIntroSequence() {
        startIntroFaceDetection();
        playIntroTTS();
        introTimeout = setTimeout(() => {
          stopIntroFaceDetection();
        }, 5000);
      }

      /* ----------------------------------------------------------------
         (B) MULTI-LANGUAGE SETUP (INCLUDING NEW MODULES)
      ---------------------------------------------------------------- */
      const translations = {
        EN: {
          greeting: "Hello, I'm AI'Han!",
          portalTitle: "AI'Han Academy - Computer Vision Lab",
          howItWorksTitle: "How Our Platform Works",
          platformDesc:
            "Our lab leverages cutting-edge AI and machine learning technologies to integrate state-of-the-art modules including Body Pose Analysis, Hand Gesture Detection, Emotion Analysis, Face Recognition & Speech Estimation, Object Detection, Motion Detection, Fun Icon Overlay, and Emoji Party. Each module is built upon advanced algorithms that push the boundaries of computer vision.",
          bodyPoseInfo: "Body Pose Analysis: Powered by TensorFlow.js and MoveNet.",
          handGestureInfo: "Hand Gesture Detection: Utilizing MediaPipe Hands.",
          emotionInfo: "Emotion Analysis: With face-api.js.",
          faceRecogInfo: "Face Recognition & Speech Estimation: This module uses MediaPipe Face Mesh.",
          platformConclusion: "Explore our platform to experience the transformative power of computer vision.",
          cvWhatTitle: "What is Computer Vision?",
          cvWhatContent: "Computer Vision is a branch of artificial intelligence that enables computers to interpret and understand digital images and videos. It utilizes image processing, machine learning, and deep learning techniques to perform tasks such as object detection, classification, and tracking.",
          cvAITitle: "Which AI Domain Does It Belong To?",
          cvAIContent: "Computer Vision is a vital subfield of AI that intersects with machine learning and deep learning. It enables systems to analyze visual data, making it crucial for applications in robotics, autonomous vehicles, and surveillance.",
          cvHistoryTitle: "History and Evolution",
          cvHistoryContent: "The field of Computer Vision began in the 1960s with basic image processing techniques. With the rise of computational power and deep learning, it has evolved dramatically. BilgisayarlÄ± gÃ¶rÃ¼, 1960â€™larda temel gÃ¶rÃ¼ntÃ¼ iÅŸleme teknikleriyle baÅŸlamÄ±ÅŸ, sonrasÄ±nda hesaplama gÃ¼cÃ¼ndeki artÄ±ÅŸ ve derin Ã¶ÄŸrenme ile devrim niteliÄŸinde geliÅŸmeler yaÅŸamÄ±ÅŸtÄ±r. It now encompasses advanced tasks such as object recognition and scene understanding.",
          cvPythonTitle: "Python Code Examples",
          cvPythonContent: "Below are detailed Python examples demonstrating key computer vision techniques.",
          cvSourcesTitle: "Sources",
          cvSourcesContent: `<ul>
  <li><a href="https://www.coursera.org/specializations/computer-vision" target="_blank">Coursera - Computer Vision Specialization</a>: A comprehensive program covering advanced computer vision techniques.</li>
  <li><a href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" target="_blank">Udacity - Computer Vision Nanodegree</a>: Hands-on projects and real-world applications in computer vision.</li>
  <li><a href="https://github.com/opencv/opencv" target="_blank">OpenCV GitHub</a>: The open source repository for the widely-used computer vision library.</li>
  <li><a href="https://www.tensorflow.org/" target="_blank">TensorFlow</a>: A robust platform for developing deep learning models.</li>
  <li><a href="https://mediapipe.dev/" target="_blank">MediaPipe</a>: An innovative framework for real-time computer vision solutions.</li>
</ul>`,
          bodyPoseTitle: "Body Pose Analysis",
          bodyTaskMsg: "Task: Raise your right arm!",
          startBtn: "Start",
          stopBtn: "Stop",
          shareBtn: "Share!",
          howItWorksBtn: "How it Works?",
          bodyHowItWorksTitle: "Body Pose Analysis",
          bodyHowItWorksTextEN: "This module uses TensorFlow.js and the MoveNet model to accurately detect and track body keypoints in real time.",
          bodyHowItWorksTextTR: "Bu modÃ¼l, TensorFlow.js ve MoveNet modeli kullanarak vÃ¼cudunuzun ana noktalarÄ±nÄ± gerÃ§ek zamanlÄ± olarak algÄ±lar ve izler.",
          handTitle: "Hand Gesture Detection",
          handTaskMsg: "Task: Wave your hand!",
          handHowItWorksTitle: "Hand Gesture Detection",
          handHowItWorksTextEN: "This module uses MediaPipe Hands to analyze your finger positions and recognize gestures.",
          handHowItWorksTextTR: "Bu modÃ¼l, MediaPipe Hands kullanarak parmak pozisyonlarÄ±nÄ±zÄ± analiz eder ve belirli hareketleri algÄ±lar.",
          emotionTitle: "Emotion Analysis",
          emotionTaskMsg: "Task: Show your facial expression!",
          emotionChartTitle: "Emotion Distribution Graph",
          emotionHowItWorksTitle: "Emotion Analysis",
          emotionHowItWorksTextEN: "This module uses face-api.js along with deep learning models to analyze your facial expressions in real time.",
          emotionHowItWorksTextTR: "Bu modÃ¼l, face-api.js ve derin Ã¶ÄŸrenme modellerini kullanarak yÃ¼z ifadenizi gerÃ§ek zamanlÄ± olarak analiz eder.",
          faceRecogTitle: "Face Recognition & Speech Estimation",
          faceSpeechMsg: "Speech Estimation: (Simulation: Speaking...)",
          faceHowItWorksTitle: "Face Recognition (Face Mesh) & Speech Estimation",
          faceHowItWorksTextEN: "This module uses MediaPipe Face Mesh to detect 468 facial landmarks and simulates speech estimation based on facial movements.",
          faceHowItWorksTextTR: "Bu modÃ¼l, MediaPipe Face Mesh kullanarak 468 yÃ¼z iÅŸaret noktasÄ±nÄ± tespit eder ve yÃ¼z hareketlerine dayalÄ± konuÅŸma tahmini simÃ¼lasyonu gerÃ§ekleÅŸtirir.",
          objectTitle: "Object Detection (COCO-SSD)",
          objectTaskMsg: "Task: Show an object in front of your camera!",
          objectHowItWorksTitle: "Object Detection (COCO-SSD)",
          objectHowItWorksTextEN: "This module utilizes the COCO-SSD model to detect various objects in real time.",
          objectHowItWorksTextTR: "Bu modÃ¼l, COCO-SSD modelini kullanarak gerÃ§ek zamanlÄ± olarak Ã§eÅŸitli nesneleri tespit eder.",
          motionTitle: "Motion Detection (Background Subtraction)",
          motionTaskMsg: "Task: Move or wave something to trigger motion detection!",
          motionHowItWorksTitle: "Motion Detection (Background Subtraction)",
          motionHowItWorksTextEN: "This module implements a simple background subtraction algorithm to detect motion.",
          motionHowItWorksTextTR: "Bu modÃ¼l, arka plan Ã§Ä±karma algoritmasÄ± kullanarak hareketi tespit eder.",
          helpModalTitle: "Help & FAQ",
          faqDataSecurity: "Data Security and Privacy",
          faqDataSecurityDesc: "This application runs solely in your browser and does not store any personal data on remote servers.",
          faqBodyPose: "How does Body Pose Analysis work?",
          faqBodyPoseDesc: "Using TensorFlow.js and MoveNet, your bodyâ€™s key points are detected and analyzed in real time.",
          faqHandGesture: "How does Hand Gesture Detection work?",
          faqHandGestureDesc: "MediaPipe Hands analyzes your finger positions to recognize gestures.",
          faqEmotion: "How does Emotion Analysis work?",
          faqEmotionDesc: "Face-api.js along with deep learning models analyze your facial expression in real time.",
          faqFaceRecog: "How does Face Recognition & Speech Estimation work?",
          faqFaceRecogDesc: "MediaPipe Face Mesh detects 468 facial landmarks to perform face recognition and simulate speech estimation.",
          faqPerformance: "Performance and Model Loading",
          faqPerformanceDesc: "All models load asynchronously in the background using WebGL acceleration to ensure optimal performance.",
          faqCloseBtn: "Close",
          faqTriggerBtn: "Help / FAQ",
          iconOverlayTitle: "Fun Icon Overlay",
          iconOverlayTaskMsg: "Task: Enjoy the fun icons overlay!",
          iconOverlayHowItWorksTitle: "Fun Icon Overlay",
          iconOverlayHowItWorksTextEN: "This module uses your camera feed to overlay fun icons that add a playful twist. A level system increases your excitement by shortening the icon update interval as you level up.",
          iconOverlayHowItWorksTextTR: "Bu modÃ¼l, kameranÄ±zÄ±n gÃ¶rÃ¼ntÃ¼sÃ¼ne eÄŸlenceli ikonlar ekler. Seviye sistemi sayesinde, seviye yÃ¼kseldikÃ§e ikon gÃ¼ncelleme aralÄ±ÄŸÄ± kÄ±salÄ±r ve deneyiminiz daha dinamik hale gelir.",
          emojiPartyTitle: "Emoji Party",
          emojiPartyTaskMsg: "Task: Let the emoji party begin!",
          emojiPartyHowItWorksTitle: "Emoji Party",
          emojiPartyHowItWorksTextEN: "This module transforms your video feed into an interactive emoji party by using Face Mesh and Hands detection to trigger tasks such as turning your head, blinking, or placing your hand on your head. When a task is recognized, the corresponding emoji is displayed for 2 seconds before moving on.",
          emojiPartyHowItWorksTextTR: "Bu modÃ¼l, Face Mesh ve Hands tespiti ile baÅŸÄ±nÄ±zÄ± Ã§evirme, gÃ¶z kÄ±rpma veya elinizi baÅŸÄ±nÄ±za koyma gibi gÃ¶revleri algÄ±lar and once the task is completed, displays the corresponding emoji for 2 seconds before moving on."
        },
        TR: {
          greeting: "Merhaba, ben AI'Han!",
          portalTitle: "AI'Han Academy - BilgisayarlÄ± GÃ¶rÃ¼ LaboratuvarÄ±",
          howItWorksTitle: "Platformumuz NasÄ±l Ã‡alÄ±ÅŸÄ±r?",
          platformDesc:
            "LaboratuvarÄ±mÄ±z, yapay zeka ve makine Ã¶ÄŸrenimi teknolojilerini kullanarak; VÃ¼cut Pose Analizi, El Hareketi Tespiti, Duygu Analizi, YÃ¼z TanÄ±ma & KonuÅŸma Tahmini, Nesne Tespiti, Hareket Tespiti, EÄŸlenceli Ä°kon Ã–rtÃ¼sÃ¼ ve Emojili Parti gibi ileri dÃ¼zey modÃ¼lleri entegre eder. Her modÃ¼l, bilgisayarlÄ± gÃ¶rÃ¼ alanÄ±nda mÃ¼mkÃ¼n olanÄ±n Ã¶tesine geÃ§meyi hedefleyen geliÅŸmiÅŸ algoritmalar Ã¼zerine kuruludur.",
          bodyPoseInfo: "VÃ¼cut Pose Analizi: TensorFlow.js ve MoveNet ile gÃ¼Ã§lendirilmiÅŸtir.",
          handGestureInfo: "El Hareketi Tespiti: MediaPipe Hands kullanÄ±larak gerÃ§ekleÅŸtirilir.",
          emotionInfo: "Duygu Analizi: face-api.js ile yÃ¼z ifadeniz analiz edilir.",
          faceRecogInfo: "YÃ¼z TanÄ±ma & KonuÅŸma Tahmini: Bu modÃ¼l, MediaPipe Face Mesh kullanÄ±r.",
          platformConclusion: "Platformumuzu keÅŸfedin ve bilgisayarlÄ± gÃ¶rÃ¼ teknolojilerinin gÃ¼cÃ¼nÃ¼ deneyimleyin.",
          cvWhatTitle: "Computer Vision Nedir?",
          cvWhatContent: "Computer Vision, bilgisayarlarÄ±n dijital gÃ¶rÃ¼ntÃ¼leri ve videolarÄ± yorumlayÄ±p anlamasÄ±nÄ± saÄŸlayan bir yapay zeka dalÄ±dÄ±r. GÃ¶rÃ¼ntÃ¼ iÅŸleme, nesne tanÄ±ma ve sahne analizi gibi teknikleri iÃ§erir.",
          cvAITitle: "Hangi Yapay Zeka Konusuna Girer?",
          cvAIContent: "Computer Vision, makine Ã¶ÄŸrenmesi ve derin Ã¶ÄŸrenmenin Ã¶nemli bir alt dalÄ±dÄ±r. Bu alan, gÃ¶rsel verinin analizinde bÃ¼yÃ¼k rol oynar ve otonom sistemler, robotik ve gÃ¼venlik uygulamalarÄ± iÃ§in kritik Ã¶neme sahiptir.",
          cvHistoryTitle: "GeliÅŸimi ve TarihÃ§esi",
          cvHistoryContent: "BilgisayarlÄ± gÃ¶rÃ¼ alanÄ±, 1960â€™larda temel gÃ¶rÃ¼ntÃ¼ iÅŸleme teknikleriyle baÅŸlamÄ±ÅŸ; hesaplama gÃ¼cÃ¼ndeki artÄ±ÅŸ ve derin Ã¶ÄŸrenme ile devrim niteliÄŸinde geliÅŸmeler yaÅŸamÄ±ÅŸtÄ±r. BilgisayarlarÄ±n dijital gÃ¶rÃ¼ntÃ¼leri yorumlayÄ±p anlamasÄ±, gÃ¼nÃ¼mÃ¼zde birÃ§ok uygulamanÄ±n temelini oluÅŸturmaktadÄ±r.",
          cvPythonTitle: "Python Kod Ã–rnekleri",
          cvPythonContent: "AÅŸaÄŸÄ±da, bilgisayarlÄ± gÃ¶rÃ¼ tekniklerini gÃ¶steren detaylÄ± Python kod Ã¶rnekleri yer almaktadÄ±r.",
          cvSourcesTitle: "Kaynaklar",
          cvSourcesContent: `<ul>
  <li><a href="https://www.coursera.org/specializations/computer-vision" target="_blank">Coursera - Computer Vision Specialization</a>: GeliÅŸmiÅŸ bilgisayarlÄ± gÃ¶rÃ¼ tekniklerini kapsayan kapsamlÄ± bir program.</li>
  <li><a href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" target="_blank">Udacity - Computer Vision Nanodegree</a>: GerÃ§ek dÃ¼nya projeleri ve uygulamalÄ± Ã¶rneklerle dolu detaylÄ± eÄŸitim.</li>
  <li><a href="https://github.com/opencv/opencv" target="_blank">OpenCV GitHub</a>: GÃ¶rÃ¼ntÃ¼ iÅŸleme ve bilgisayarlÄ± gÃ¶rÃ¼ uygulamalarÄ± iÃ§in en popÃ¼ler aÃ§Ä±k kaynak kÃ¼tÃ¼phanesi.</li>
  <li><a href="https://www.tensorflow.org/" target="_blank">TensorFlow</a>: Derin Ã¶ÄŸrenme modellerinin geliÅŸtirilmesinde kullanÄ±lan geniÅŸ Ã§aplÄ± bir platform.</li>
  <li><a href="https://mediapipe.dev/" target="_blank">MediaPipe</a>: GerÃ§ek zamanlÄ± gÃ¶rÃ¼ntÃ¼ iÅŸleme Ã§Ã¶zÃ¼mleri sunan yenilikÃ§i bir framework.</li>
</ul>`,
          bodyPoseTitle: "VÃ¼cut Pose Analizi",
          bodyTaskMsg: "GÃ¶rev: SaÄŸ kolunuzu kaldÄ±rÄ±n!",
          startBtn: "BaÅŸlat",
          stopBtn: "Durdur",
          shareBtn: "PaylaÅŸ!",
          howItWorksBtn: "NasÄ±l Ã‡alÄ±ÅŸÄ±r?",
          bodyHowItWorksTitle: "VÃ¼cut Pose Analizi",
          bodyHowItWorksTextEN: "This module uses TensorFlow.js and the MoveNet model to accurately detect and track body keypoints in real time.",
          bodyHowItWorksTextTR: "Bu modÃ¼l, TensorFlow.js ve MoveNet modeli kullanarak vÃ¼cudunuzun ana noktalarÄ±nÄ± gerÃ§ek zamanlÄ± olarak algÄ±lar ve izler.",
          handTitle: "El Hareketi Tespiti",
          handTaskMsg: "GÃ¶rev: Elinizi sallayÄ±n!",
          handHowItWorksTitle: "El Hareketi Tespiti",
          handHowItWorksTextEN: "This module uses MediaPipe Hands to analyze your finger positions and recognize gestures.",
          handHowItWorksTextTR: "Bu modÃ¼l, MediaPipe Hands kullanarak parmak pozisyonlarÄ±nÄ±zÄ± analiz eder ve belirli hareketleri algÄ±lar.",
          emotionTitle: "Duygu Analizi",
          emotionTaskMsg: "GÃ¶rev: YÃ¼z ifadenizi gÃ¶sterin!",
          emotionChartTitle: "Duygu DaÄŸÄ±lÄ±m GrafiÄŸi",
          emotionHowItWorksTitle: "Duygu Analizi",
          emotionHowItWorksTextEN: "This module uses face-api.js along with deep learning models to analyze your facial expressions in real time.",
          emotionHowItWorksTextTR: "Bu modÃ¼l, face-api.js ve derin Ã¶ÄŸrenme modellerini kullanarak yÃ¼z ifadenizi gerÃ§ek zamanlÄ± olarak analiz eder.",
          faceRecogTitle: "YÃ¼z TanÄ±ma & KonuÅŸma Tahmini",
          faceSpeechMsg: "KonuÅŸma Tahmini: (SimÃ¼lasyon: KonuÅŸuyor...)",
          faceHowItWorksTitle: "YÃ¼z TanÄ±ma (Face Mesh) & KonuÅŸma Tahmini",
          faceHowItWorksTextEN: "This module uses MediaPipe Face Mesh to detect 468 facial landmarks and simulates speech estimation based on facial movements.",
          faceHowItWorksTextTR: "Bu modÃ¼l, MediaPipe Face Mesh kullanarak 468 yÃ¼z iÅŸaret noktasÄ±nÄ± tespit eder ve yÃ¼z hareketlerine dayalÄ± konuÅŸma tahmini simÃ¼lasyonu gerÃ§ekleÅŸtirir.",
          objectTitle: "Nesne Tespiti (COCO-SSD)",
          objectTaskMsg: "GÃ¶rev: KameranÄ±zÄ±n Ã¶nÃ¼ne bir nesne tutun!",
          objectHowItWorksTitle: "Nesne Tespiti (COCO-SSD)",
          objectHowItWorksTextEN: "This module utilizes the COCO-SSD model to detect various objects in real time.",
          objectHowItWorksTextTR: "Bu modÃ¼l, COCO-SSD modelini kullanarak gerÃ§ek zamanlÄ± olarak Ã§eÅŸitli nesneleri tespit eder.",
          motionTitle: "Hareket Tespiti (Arka Plan Ã‡Ä±karma)",
          motionTaskMsg: "GÃ¶rev: Hareket veya bir cisim sallayarak tespiti deneyin!",
          motionHowItWorksTitle: "Hareket Tespiti (Arka Plan Ã‡Ä±karma)",
          motionHowItWorksTextEN: "This module implements a simple background subtraction algorithm to detect motion.",
          motionHowItWorksTextTR: "Bu modÃ¼l, arka plan Ã§Ä±karma algoritmasÄ± kullanarak hareketi tespit eder.",
          helpModalTitle: "YardÄ±m & SSS",
          faqDataSecurity: "Veri GÃ¼venliÄŸi ve Gizlilik",
          faqDataSecurityDesc: "Uygulama yalnÄ±zca tarayÄ±cÄ±nÄ±zda Ã§alÄ±ÅŸÄ±r ve kiÅŸisel verilerinizi uzak sunucularda saklamaz.",
          faqBodyPose: "VÃ¼cut Pose Analizi nasÄ±l Ã§alÄ±ÅŸÄ±r?",
          faqBodyPoseDesc: "TensorFlow.js ve MoveNet ile vÃ¼cudunuzun ana noktalarÄ± tespit edilir ve gerÃ§ek zamanlÄ± analiz yapÄ±lÄ±r.",
          faqHandGesture: "El Hareketi Tespiti nasÄ±l Ã§alÄ±ÅŸÄ±r?",
          faqHandGestureDesc: "MediaPipe Hands, parmak konumlarÄ±nÄ±zÄ± analiz ederek el hareketlerinizi algÄ±lar.",
          faqEmotion: "Duygu Analizi nasÄ±l Ã§alÄ±ÅŸÄ±r?",
          faqEmotionDesc: "face-api.js ve derin Ã¶ÄŸrenme modelleri kullanÄ±larak yÃ¼z ifadeniz gerÃ§ek zamanlÄ± olarak analiz edilir.",
          faqFaceRecog: "YÃ¼z TanÄ±ma & KonuÅŸma Tahmini nasÄ±l Ã§alÄ±ÅŸÄ±r?",
          faqFaceRecogDesc: "MediaPipe Face Mesh, 468 yÃ¼z iÅŸaret noktasÄ±nÄ± tespit ederek yÃ¼z tanÄ±ma ve konuÅŸma tahmini gerÃ§ekleÅŸtirir.",
          faqPerformance: "Performans ve Model YÃ¼kleme",
          faqPerformanceDesc: "TÃ¼m modeller, WebGL hÄ±zlandÄ±rmasÄ± ile arka planda asenkron olarak yÃ¼klenir.",
          faqCloseBtn: "Kapat",
          faqTriggerBtn: "YardÄ±m / SSS",
          iconOverlayTitle: "EÄŸlenceli Ä°kon Ã–rtÃ¼sÃ¼",
          iconOverlayTaskMsg: "GÃ¶rev: EÄŸlenceli ikon Ã¶rtÃ¼sÃ¼nÃ¼n keyfini Ã§Ä±karÄ±n!",
          iconOverlayHowItWorksTitle: "EÄŸlenceli Ä°kon Ã–rtÃ¼sÃ¼",
          iconOverlayHowItWorksTextEN: "This module uses your camera feed to overlay fun icons that add a playful twist. A level system increases your excitement by shortening the icon update interval as you level up.",
          iconOverlayHowItWorksTextTR: "Bu modÃ¼l, kameranÄ±zÄ±n gÃ¶rÃ¼ntÃ¼sÃ¼ne eÄŸlenceli ikonlar ekler. Entegre seviye sistemi sayesinde, seviye yÃ¼kseldikÃ§e ikon gÃ¼ncelleme aralÄ±ÄŸÄ± kÄ±salÄ±r ve deneyiminiz daha dinamik hale gelir.",
          emojiPartyTitle: "Emojili Parti",
          emojiPartyTaskMsg: "GÃ¶rev: Emojili partiyi baÅŸlatÄ±n!",
          emojiPartyHowItWorksTitle: "Emojili Parti",
          emojiPartyHowItWorksTextEN: "This module transforms your video feed into an interactive emoji party by using Face Mesh and Hands detection to trigger tasks such as turning your head, blinking, or placing your hand on your head. When a task is recognized, the corresponding emoji is displayed for 2 seconds before moving on.",
          emojiPartyHowItWorksTextTR: "Bu modÃ¼l, Face Mesh ve Hands tespiti ile baÅŸÄ±nÄ±zÄ± Ã§evirme, gÃ¶z kÄ±rpma veya elinizi baÅŸÄ±nÄ±za koyma gibi gÃ¶revleri algÄ±lar and once the task is completed, displays the corresponding emoji for 2 seconds before moving on."
        }
      };

      let currentLang = "EN"; // default

      function setLanguage(lang) {
        currentLang = lang;
        // GÃ¼ncellenen kÄ±sÄ±m: eÄŸer Ã§eviri metninde "<" karakteri varsa innerHTML, yoksa textContent kullan
        document.querySelectorAll("[data-translate]").forEach((elem) => {
          const key = elem.getAttribute("data-translate");
          if (translations[lang][key]) {
            if (translations[lang][key].indexOf("<") !== -1) {
              elem.innerHTML = translations[lang][key];
            } else {
              elem.textContent = translations[lang][key];
            }
          }
        });
        // Dynamic contents for educational modules
        document.getElementById("bodyHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='bodyHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='bodyHowItWorksTextTR']").textContent;
        document.getElementById("handHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='handHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='handHowItWorksTextTR']").textContent;
        document.getElementById("emotionHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='emotionHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='emotionHowItWorksTextTR']").textContent;
        document.getElementById("faceHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='faceHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='faceHowItWorksTextTR']").textContent;
        document.getElementById("objectHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='objectHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='objectHowItWorksTextTR']").textContent;
        document.getElementById("motionHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='motionHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='motionHowItWorksTextTR']").textContent;
        // New modules dynamic texts
        document.getElementById("iconOverlayHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='iconOverlayHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='iconOverlayHowItWorksTextTR']").textContent;
        document.getElementById("emojiPartyHowItWorksDynamic").innerHTML =
          lang === "EN"
            ? document.querySelector("[data-translate='emojiPartyHowItWorksTextEN']").textContent
            : document.querySelector("[data-translate='emojiPartyHowItWorksTextTR']").textContent;
      }

      document.getElementById("btnEN").addEventListener("click", () => setLanguage("EN"));
      document.getElementById("btnTR").addEventListener("click", () => setLanguage("TR"));
      setLanguage("EN");

      /* ----------------------------------------------------------------
         (C) SOUND & TTS
      ---------------------------------------------------------------- */
      function playClickSound() {
        const clickSound = document.getElementById("clickSound");
        if (clickSound) {
          clickSound.currentTime = 0;
          clickSound.play();
        }
      }

      function announceTask(messageEN, messageTR) {
        const utterance = new SpeechSynthesisUtterance(
          currentLang === "EN" ? messageEN : messageTR
        );
        speechSynthesis.speak(utterance);
      }

      /* ----------------------------------------------------------------
         (D) BODY POSE ANALYSIS
      ---------------------------------------------------------------- */
      let bodyDetector = null;
      let bodyDetectionFrame;
      let bodyFrameCount = 0;
      let bodyLastTime = performance.now();
      let bodyTaskCompleted = false;
      const bodyVideo = document.getElementById("bodyVideo");
      const bodyCanvas = document.getElementById("bodyCanvas");
      const bodyCtx = bodyCanvas.getContext("2d");
      const bodyLoader = document.getElementById("bodyLoader");
      const bodyFpsSpan = document.getElementById("bodyFps");
      const bodyLandmarksSpan = document.getElementById("bodyLandmarks");
      const bodyTaskEl = document.getElementById("bodyTask");

      const bodyTasks = [
        {
          type: "right_arm",
          messageEN: "Task: Raise your right arm!",
          messageTR: "GÃ¶rev: SaÄŸ kolunuzu kaldÄ±rÄ±n!",
        },
        {
          type: "left_arm",
          messageEN: "Task: Raise your left arm!",
          messageTR: "GÃ¶rev: Sol kolunuzu kaldÄ±rÄ±n!",
        }
      ];
      let currentBodyTaskIndex = 0;

      function nextBodyTask() {
        currentBodyTaskIndex = (currentBodyTaskIndex + 1) % bodyTasks.length;
        bodyTaskCompleted = false;
        bodyTaskEl.textContent = currentLang === "EN"
          ? bodyTasks[currentBodyTaskIndex].messageEN
          : bodyTasks[currentBodyTaskIndex].messageTR;
        bodyTaskEl.style.color = "var(--primary-color)";
      }

      function calculateAngle(a, b, c) {
        const ab = { x: b.x - a.x, y: b.y - a.y };
        const cb = { x: b.x - c.x, y: b.y - c.y };
        const dot = ab.x * cb.x + ab.y * cb.y;
        const magAB = Math.sqrt(ab.x ** 2 + ab.y ** 2);
        const magCB = Math.sqrt(cb.x ** 2 + cb.y ** 2);
        const cosAngle = dot / (magAB * magCB);
        const angle = Math.acos(cosAngle);
        return angle * (180 / Math.PI);
      }

      function drawSkeleton(ctx, keypoints) {
        const connectPairs = [
          [5, 6],
          [5, 7],
          [7, 9],
          [6, 8],
          [8, 10],
          [5, 11],
          [6, 12],
          [11, 12],
          [11, 13],
          [13, 15],
          [12, 14],
          [14, 16],
        ];
        ctx.strokeStyle = "blue";
        ctx.lineWidth = 2;
        connectPairs.forEach(([i1, i2]) => {
          const kp1 = keypoints[i1];
          const kp2 = keypoints[i2];
          if (kp1.score > 0.5 && kp2.score > 0.5) {
            ctx.beginPath();
            ctx.moveTo(kp1.x, kp1.y);
            ctx.lineTo(kp2.x, kp2.y);
            ctx.stroke();
          }
        });
      }

      async function initBodyDetector() {
        bodyLoader.style.display = "block";
        const model = poseDetection.SupportedModels.MoveNet;
        bodyDetector = await poseDetection.createDetector(model, {
          modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING,
        });
        bodyLoader.style.display = "none";
      }

      async function detectBodyPose() {
        if (!bodyDetector) return;
        const poses = await bodyDetector.estimatePoses(bodyVideo);
        bodyCtx.clearRect(0, 0, bodyCanvas.width, bodyCanvas.height);

        if (poses && poses.length > 0) {
          const keypoints = poses[0].keypoints;
          keypoints.forEach((kp) => {
            if (kp.score > 0.5) {
              bodyCtx.beginPath();
              bodyCtx.arc(kp.x, kp.y, 5, 0, 2 * Math.PI);
              bodyCtx.fillStyle = "red";
              bodyCtx.fill();
            }
          });
          drawSkeleton(bodyCtx, keypoints);
          bodyLandmarksSpan.textContent = "Landmarks: " + keypoints.filter((p) => p.score > 0.5).length;

          const task = bodyTasks[currentBodyTaskIndex];
          let wristIndex, elbowIndex, shoulderIndex;
          if (task.type === "right_arm") {
            wristIndex = 10;
            elbowIndex = 8;
            shoulderIndex = 6;
          } else {
            wristIndex = 9;
            elbowIndex = 7;
            shoulderIndex = 5;
          }
          const wrist = keypoints[wristIndex];
          const elbow = keypoints[elbowIndex];
          const shoulder = keypoints[shoulderIndex];

          if (
            wrist.score > 0.5 && elbow.score > 0.5 && shoulder.score > 0.5 &&
            !bodyTaskCompleted
          ) {
            const angle = calculateAngle(shoulder, elbow, wrist);
            if (angle > 150 && wrist.y < shoulder.y) {
              const msgEN = `Task Completed: ${task.type === "right_arm" ? "Right" : "Left"} arm raised! (Angle: ${angle.toFixed(0)}Â°)`;
              const msgTR = `GÃ¶rev BaÅŸarÄ±lÄ±: ${task.type === "right_arm" ? "SaÄŸ" : "Sol"} kol kaldÄ±rÄ±ldÄ±! (AÃ§Ä±: ${angle.toFixed(0)}Â°)`;
              bodyTaskEl.textContent = currentLang === "EN" ? msgEN : msgTR;
              bodyTaskEl.style.color = "green";
              bodyTaskCompleted = true;
              confetti({ particleCount: 100, spread: 60, origin: { y: 0.5 } });
              announceTask(msgEN, msgTR);
              setTimeout(nextBodyTask, 3000);
            }
          }
        }

        bodyFrameCount++;
        const now = performance.now();
        if (now - bodyLastTime >= 1000) {
          bodyFpsSpan.textContent = "FPS: " + bodyFrameCount;
          bodyFrameCount = 0;
          bodyLastTime = now;
        }
        bodyDetectionFrame = requestAnimationFrame(detectBodyPose);
      }

      async function startBodyPoseDetection() {
        playClickSound();
        if (!bodyDetector) await initBodyDetector();
        currentBodyTaskIndex = 0;
        bodyTaskCompleted = false;
        bodyTaskEl.textContent = currentLang === "EN"
          ? bodyTasks[currentBodyTaskIndex].messageEN
          : bodyTasks[currentBodyTaskIndex].messageTR;
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          bodyVideo.srcObject = stream;
          await bodyVideo.play();
          bodyCanvas.width = bodyVideo.videoWidth;
          bodyCanvas.height = bodyVideo.videoHeight;
          detectBodyPose();
        } catch (err) {
          console.error("Body detection camera error:", err);
        }
      }

      function stopBodyPoseDetection() {
        playClickSound();
        cancelAnimationFrame(bodyDetectionFrame);
        if (bodyVideo.srcObject) {
          bodyVideo.srcObject.getTracks().forEach((t) => t.stop());
          bodyVideo.srcObject = null;
        }
      }

      /* ----------------------------------------------------------------
         (E) HAND GESTURE DETECTION (Storing globalHandLandmarks)
      ---------------------------------------------------------------- */
      let mpHands = null;
      let handCamera = null;
      let handFrameCount = 0;
      let handLastTime = performance.now();
      let handTaskCompleted = false;
      const handVideo = document.getElementById("handVideo");
      const handCanvas = document.getElementById("handCanvas");
      const handCtx = handCanvas.getContext("2d");
      const handLoader = document.getElementById("handLoader");
      const handFpsSpan = document.getElementById("handFps");
      const handLandmarksSpan = document.getElementById("handLandmarks");
      const handTaskEl = document.getElementById("handTask");

      // YENÄ° EKLENEN: KÃ¼resel el landmark deÄŸiÅŸkeni
      let globalHandLandmarks = null;

      const handTasks = [
        {
          messageEN: "Task: Wave your hand!",
          messageTR: "GÃ¶rev: Elinizi sallayÄ±n!"
        }
      ];
      let currentHandTaskIndex = 0;

      function nextHandTaskHand() {
        currentHandTaskIndex = (currentHandTaskIndex + 1) % handTasks.length;
        handTaskCompleted = false;
        handTaskEl.textContent = currentLang === "EN"
          ? handTasks[currentHandTaskIndex].messageEN
          : handTasks[currentHandTaskIndex].messageTR;
        handTaskEl.style.color = "var(--primary-color)";
      }

      function initHandDetector() {
        handLoader.style.display = "block";
        mpHands = new Hands({
          locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
        });
        mpHands.setOptions({
          maxNumHands: 1,
          modelComplexity: 0,
          minDetectionConfidence: 0.3,
          minTrackingConfidence: 0.3,
        });
        mpHands.onResults(onHandResults);
        handLoader.style.display = "none";
      }

      function onHandResults(results) {
        handCtx.clearRect(0, 0, handCanvas.width, handCanvas.height);

        if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
          const landmarks = results.multiHandLandmarks[0];
          globalHandLandmarks = landmarks; // atama

          // Draw landmarks
          landmarks.forEach((pt) => {
            handCtx.beginPath();
            handCtx.arc(pt.x * handCanvas.width, pt.y * handCanvas.height, 4, 0, 2 * Math.PI);
            handCtx.fillStyle = "green";
            handCtx.fill();
          });
          // Draw connections
          for (let c of mpHands.HAND_CONNECTIONS) {
            const [start, end] = c;
            const pt1 = landmarks[start];
            const pt2 = landmarks[end];
            handCtx.beginPath();
            handCtx.moveTo(pt1.x * handCanvas.width, pt1.y * handCanvas.height);
            handCtx.lineTo(pt2.x * handCanvas.width, pt2.y * handCanvas.height);
            handCtx.strokeStyle = "blue";
            handCtx.lineWidth = 2;
            handCtx.stroke();
          }
          handLandmarksSpan.textContent = "Landmarks: " + landmarks.length;

          // Basit dalga tespiti
          if (!handTaskCompleted) {
            handTaskCompleted = true;
            const msgEN = "Task Completed: Hand waved!";
            const msgTR = "GÃ¶rev BaÅŸarÄ±lÄ±: El sallama algÄ±landÄ±!";
            handTaskEl.textContent = currentLang === "EN" ? msgEN : msgTR;
            handTaskEl.style.color = "green";
            confetti({ particleCount: 100, spread: 60, origin: { y: 0.5 } });
            announceTask(msgEN, msgTR);
            setTimeout(nextHandTaskHand, 3000);
          }
        } else {
          globalHandLandmarks = null;
          handLandmarksSpan.textContent = "Landmarks: 0";
        }

        handFrameCount++;
        const now = performance.now();
        if (now - handLastTime >= 1000) {
          handFpsSpan.textContent = "FPS: " + handFrameCount;
          handFrameCount = 0;
          handLastTime = now;
        }
      }

      async function startHandPoseDetection() {
        playClickSound();
        if (!mpHands) initHandDetector();
        currentHandTaskIndex = 0;
        handTaskCompleted = false;
        handTaskEl.textContent = currentLang === "EN"
          ? handTasks[currentHandTaskIndex].messageEN
          : handTasks[currentHandTaskIndex].messageTR;
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          handVideo.srcObject = stream;
          await handVideo.play();
          handCanvas.width = handVideo.videoWidth;
          handCanvas.height = handVideo.videoHeight;
          handCamera = new Camera(handVideo, {
            onFrame: async () => {
              await mpHands.send({ image: handVideo });
            },
            width: handVideo.videoWidth,
            height: handVideo.videoHeight,
          });
          handCamera.start();
        } catch (err) {
          console.error("Hand detection camera error:", err);
        }
      }

      function stopHandPoseDetection() {
        playClickSound();
        if (handCamera) {
          handCamera.stop();
          handCamera = null;
        }
        if (handVideo.srcObject) {
          handVideo.srcObject.getTracks().forEach((t) => t.stop());
          handVideo.srcObject = null;
        }
      }

      /* ----------------------------------------------------------------
         (F) EMOTION ANALYSIS (face-api.js)
      ---------------------------------------------------------------- */
      let emotionModelsLoaded = false;
      let emotionStream = null;
      let emotionDetectionFrame;
      let emotionFrameCount = 0;
      let emotionLastTime = performance.now();
      const emotionVideo = document.getElementById("emotionVideo");
      const emotionCanvas = document.getElementById("emotionCanvas");
      const emotionCtx = emotionCanvas.getContext("2d");
      const emotionLoader = document.getElementById("emotionLoader");
      const emotionFpsSpan = document.getElementById("emotionFps");
      const emotionResultSpan = document.getElementById("emotionResult");

      let emotionChart;

      async function loadEmotionModels() {
        try {
          await faceapi.nets.tinyFaceDetector.loadFromUri("https://justadudewhohacks.github.io/face-api.js/models");
          await faceapi.nets.faceExpressionNet.loadFromUri("https://justadudewhohacks.github.io/face-api.js/models");
          await faceapi.nets.faceLandmark68Net.loadFromUri("https://justadudewhohacks.github.io/face-api.js/models");
          emotionModelsLoaded = true;
          initEmotionChart();
        } catch (error) {
          console.error("Error loading emotion models:", error);
        }
      }

      function initEmotionChart() {
        const ctx = document.getElementById("emotionChart").getContext("2d");
        emotionChart = new Chart(ctx, {
          type: "bar",
          data: {
            labels: ["angry", "disgust", "fear", "happy", "sad", "surprise", "neutral"],
            datasets: [
              {
                label: currentLang === "EN" ? "Emotion (%)" : "Duygu (%)",
                data: [0, 0, 0, 0, 0, 0, 0],
                backgroundColor: [
                  "rgba(255, 99, 132, 0.5)",
                  "rgba(255, 159, 64, 0.5)",
                  "rgba(255, 205, 86, 0.5)",
                  "rgba(75, 192, 192, 0.5)",
                  "rgba(54, 162, 235, 0.5)",
                  "rgba(153, 102, 255, 0.5)",
                  "rgba(201, 203, 207, 0.5)",
                ],
              },
            ],
          },
          options: {
            responsive: true,
            animation: { duration: 0 },
            scales: { y: { beginAtZero: true, max: 100 } },
          },
        });
      }

      async function analyzeEmotion() {
        const detection = await faceapi
          .detectSingleFace(emotionVideo, new faceapi.TinyFaceDetectorOptions())
          .withFaceExpressions();
        emotionCtx.drawImage(emotionVideo, 0, 0, emotionCanvas.width, emotionCanvas.height);

        // EK: YÃ¼zÃ¼ belirginleÅŸtirmek iÃ§in bounding box Ã§izimi (varsa)
        if (detection && detection.box) {
          const { x, y, width, height } = detection.box;
          emotionCtx.strokeStyle = "lime";
          emotionCtx.lineWidth = 2;
          emotionCtx.strokeRect(x, y, width, height);
        }

        if (detection && detection.expressions) {
          const expressions = detection.expressions;
          const maxEmotion = Object.keys(expressions).reduce((a, b) =>
            expressions[a] > expressions[b] ? a : b
          );
          const percent = (expressions[maxEmotion] * 100).toFixed(0);
          const labelEN = `Emotion: ${maxEmotion} (${percent}%)`;
          const labelTR = `Duygu: ${maxEmotion} (${percent}%)`;
          emotionCtx.fillStyle = "rgba(0,0,0,0.4)";
          emotionCtx.fillRect(0, 0, emotionCanvas.width, 40);
          emotionCtx.fillStyle = "white";
          emotionCtx.font = "20px Arial";
          emotionCtx.fillText(currentLang === "EN" ? labelEN : labelTR, 10, 28);
          emotionResultSpan.textContent = currentLang === "EN" ? labelEN : labelTR;

          // Update chart data
          emotionChart.data.datasets[0].data = [
            (expressions.angry * 100).toFixed(1),
            (expressions.disgust * 100).toFixed(1),
            (expressions.fear * 100).toFixed(1),
            (expressions.happy * 100).toFixed(1),
            (expressions.sad * 100).toFixed(1),
            (expressions.surprise * 100).toFixed(1),
            (expressions.neutral * 100).toFixed(1),
          ];
          emotionChart.update();

          if (maxEmotion === "happy" && expressions.happy > 0.8) {
            confetti({ particleCount: 120, spread: 70, origin: { y: 0.5 } });
          }
        }

        emotionFrameCount++;
        const now = performance.now();
        if (now - emotionLastTime >= 1000) {
          emotionFpsSpan.textContent = "FPS: " + emotionFrameCount;
          emotionFrameCount = 0;
          emotionLastTime = now;
        }

        emotionDetectionFrame = requestAnimationFrame(analyzeEmotion);
      }

      async function startEmotionAnalysis() {
        playClickSound();
        emotionLoader.style.display = "block";
        if (!emotionModelsLoaded) {
          await loadEmotionModels();
        }
        try {
          emotionStream = await navigator.mediaDevices.getUserMedia({ video: true });
          emotionVideo.srcObject = emotionStream;
          await emotionVideo.play();
          emotionCanvas.width = emotionVideo.videoWidth;
          emotionCanvas.height = emotionVideo.videoHeight;
          emotionLoader.style.display = "none";
          analyzeEmotion();
        } catch (err) {
          console.error("Emotion analysis camera error:", err);
        }
      }

      function stopEmotionAnalysis() {
        playClickSound();
        cancelAnimationFrame(emotionDetectionFrame);
        if (emotionStream) {
          emotionStream.getTracks().forEach((t) => t.stop());
          emotionStream = null;
        }
      }

      /* ----------------------------------------------------------------
         (F) FACE RECOGNITION & SPEECH ESTIMATION
      ---------------------------------------------------------------- */
      let faceMesh = null;
      let faceCamera = null;
      let faceFrameCount = 0;
      let faceLastTime = performance.now();
      const faceLoader = document.getElementById("faceLoader");
      const faceVideo = document.getElementById("faceVideo");
      const faceCanvas = document.getElementById("faceCanvas");
      const faceCtx = faceCanvas.getContext("2d");
      const faceFpsSpan = document.getElementById("faceFps");

      async function initFaceMesh() {
        faceLoader.style.display = "block";
        faceMesh = new FaceMesh({
          locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
        });
        faceMesh.setOptions({
          maxNumFaces: 1,
          refineLandmarks: true,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5,
        });
        faceMesh.onResults(onFaceResults);
        faceLoader.style.display = "none";
      }

      function onFaceResults(results) {
        faceCtx.clearRect(0, 0, faceCanvas.width, faceCanvas.height);
        if (results.multiFaceLandmarks) {
          const landmarks = results.multiFaceLandmarks[0];
          landmarks.forEach((pt) => {
            faceCtx.beginPath();
            faceCtx.arc(pt.x * faceCanvas.width, pt.y * faceCanvas.height, 2, 0, 2 * Math.PI);
            faceCtx.fillStyle = "orange";
            faceCtx.fill();
          });
        }
        faceFrameCount++;
        const now = performance.now();
        if (now - faceLastTime >= 1000) {
          faceFpsSpan.textContent = "FPS: " + faceFrameCount;
          faceFrameCount = 0;
          faceLastTime = now;
        }
      }

      async function startFaceDetection() {
        playClickSound();
        if (!faceMesh) await initFaceMesh();
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          faceVideo.srcObject = stream;
          await faceVideo.play();
          faceCanvas.width = faceVideo.videoWidth;
          faceCanvas.height = faceVideo.videoHeight;
          faceCamera = new Camera(faceVideo, {
            onFrame: async () => {
              await faceMesh.send({ image: faceVideo });
            },
            width: faceVideo.videoWidth,
            height: faceVideo.videoHeight,
          });
          faceCamera.start();
        } catch (err) {
          console.error("Face detection camera error:", err);
        }
      }

      function stopFaceDetection() {
        playClickSound();
        if (faceCamera) {
          faceCamera.stop();
          faceCamera = null;
        }
        if (faceVideo.srcObject) {
          faceVideo.srcObject.getTracks().forEach((t) => t.stop());
          faceVideo.srcObject = null;
        }
      }

      /* ----------------------------------------------------------------
         (G) OBJECT DETECTION (COCO-SSD)
      ---------------------------------------------------------------- */
      let objectModel = null;
      let objectDetectionFrame;
      let objectFrameCount = 0;
      let objectLastTime = performance.now();
      let objectStream = null;
      const objectVideo = document.getElementById("objectVideo");
      const objectCanvas = document.getElementById("objectCanvas");
      const objectCtx = objectCanvas.getContext("2d");
      const objectLoader = document.getElementById("objectLoader");
      const objectFpsSpan = document.getElementById("objectFps");
      const objectDetectionsSpan = document.getElementById("objectDetections");

      async function loadObjectModel() {
        objectLoader.style.display = "block";
        objectModel = await cocoSsd.load();
        objectLoader.style.display = "none";
      }

      async function detectObjects() {
        objectCtx.drawImage(objectVideo, 0, 0, objectCanvas.width, objectCanvas.height);
        const predictions = await objectModel.detect(objectCanvas);
        objectCtx.drawImage(objectVideo, 0, 0, objectCanvas.width, objectCanvas.height);
        let detectionsCount = 0;
        predictions.forEach((prediction) => {
          detectionsCount++;
          const [x, y, width, height] = prediction.bbox;
          objectCtx.beginPath();
          objectCtx.rect(x, y, width, height);
          objectCtx.lineWidth = 2;
          objectCtx.strokeStyle = "blue";
          objectCtx.fillStyle = "blue";
          objectCtx.stroke();
          objectCtx.fillStyle = "white";
          objectCtx.font = "16px Arial";
          objectCtx.fillText(
            `${prediction.class} (${(prediction.score * 100).toFixed(1)}%)`,
            x + 5,
            y > 20 ? y - 5 : y + 15
          );
        });
        objectDetectionsSpan.textContent = "Detections: " + detectionsCount;
        objectFrameCount++;
        const now = performance.now();
        if (now - objectLastTime >= 1000) {
          objectFpsSpan.textContent = "FPS: " + objectFrameCount;
          objectFrameCount = 0;
          objectLastTime = now;
        }
        objectDetectionFrame = requestAnimationFrame(detectObjects);
      }

      async function startObjectDetection() {
        playClickSound();
        if (!objectModel) await loadObjectModel();
        try {
          objectStream = await navigator.mediaDevices.getUserMedia({ video: true });
          objectVideo.srcObject = objectStream;
          await objectVideo.play();
          objectCanvas.width = objectVideo.videoWidth;
          objectCanvas.height = objectVideo.videoHeight;
          detectObjects();
        } catch (err) {
          console.error("Object detection camera error:", err);
        }
      }

      function stopObjectDetection() {
        playClickSound();
        cancelAnimationFrame(objectDetectionFrame);
        if (objectStream) {
          objectStream.getTracks().forEach((t) => t.stop());
          objectStream = null;
        }
      }

      /* ----------------------------------------------------------------
         (H) MOTION DETECTION (Background Subtraction)
      ---------------------------------------------------------------- */
      let motionDetectionFrame;
      let motionFrameCount = 0;
      let motionLastTime = performance.now();
      let motionStream = null;
      let backgroundData = null;
      const motionVideo = document.getElementById("motionVideo");
      const motionCanvas = document.getElementById("motionCanvas");
      const motionCtx = motionCanvas.getContext("2d");
      const motionLoader = document.getElementById("motionLoader");
      const motionFpsSpan = document.getElementById("motionFps");
      const motionStatusSpan = document.getElementById("motionStatus");

      async function detectMotion() {
        motionCtx.drawImage(motionVideo, 0, 0, motionCanvas.width, motionCanvas.height);
        const frameData = motionCtx.getImageData(0, 0, motionCanvas.width, motionCanvas.height);
        let motionPixels = 0;
        if (backgroundData) {
          for (let i = 0; i < frameData.data.length; i += 4) {
            const diff =
              Math.abs(frameData.data[i] - backgroundData.data[i]) +
              Math.abs(frameData.data[i + 1] - backgroundData.data[i + 1]) +
              Math.abs(frameData.data[i + 2] - backgroundData.data[i + 2]);
            if (diff > 60) {
              motionPixels++;
              // highlight changed pixels in red
              frameData.data[i] = 255;
              frameData.data[i + 1] = 0;
              frameData.data[i + 2] = 0;
            }
          }
        } else {
          backgroundData = frameData;
        }
        motionCtx.putImageData(frameData, 0, 0);
        motionStatusSpan.textContent = motionPixels > 1000
          ? "Motion: Detected"
          : "Motion: None";

        motionFrameCount++;
        const now = performance.now();
        if (now - motionLastTime >= 1000) {
          motionFpsSpan.textContent = "FPS: " + motionFrameCount;
          motionFrameCount = 0;
          motionLastTime = now;
        }
        motionDetectionFrame = requestAnimationFrame(detectMotion);
      }

      async function startMotionDetection() {
        playClickSound();
        motionLoader.style.display = "block";
        backgroundData = null;
        try {
          motionStream = await navigator.mediaDevices.getUserMedia({ video: true });
          motionVideo.srcObject = motionStream;
          await motionVideo.play();
          motionCanvas.width = motionVideo.videoWidth;
          motionCanvas.height = motionVideo.videoHeight;
          motionLoader.style.display = "none";
          detectMotion();
        } catch (err) {
          console.error("Motion detection camera error:", err);
        }
      }

      function stopMotionDetection() {
        playClickSound();
        cancelAnimationFrame(motionDetectionFrame);
        if (motionStream) {
          motionStream.getTracks().forEach((t) => t.stop());
          motionStream = null;
        }
      }

      /* ----------------------------------------------------------------
         (I) FUN ICON OVERLAY (Optimized)
      ---------------------------------------------------------------- */
      let iconOverlayStream = null;
      let iconOverlayAnimationFrame, iconOverlayInterval, iconOverlayLevelInterval;
      let iconOverlayLevel = 1;
      let currentIcon = null, currentIconX = 0, currentIconY = 0;
      const iconOverlayVideo = document.getElementById("iconOverlayVideo");
      const iconOverlayCanvas = document.getElementById("iconOverlayCanvas");
      const iconOverlayCtx = iconOverlayCanvas.getContext("2d");
      const iconOverlayLoader = document.getElementById("iconOverlayLoader");
      const iconOverlayFpsSpan = document.getElementById("iconOverlayFps");

      // Fun icon URLs
      const funIconURLs = [
        "https://via.placeholder.com/50?text=ðŸ˜Š",
        "https://via.placeholder.com/50?text=â­",
        "https://via.placeholder.com/50?text=â¤ï¸",
        "https://via.placeholder.com/50?text=ðŸ˜‚",
        "https://via.placeholder.com/50?text=ðŸŽˆ"
      ];
      const funIconImages = [];
      funIconURLs.forEach(url => {
        const img = new Image();
        img.src = url;
        funIconImages.push(img);
      });

      function updateIconOverlay() {
        currentIcon = funIconImages[Math.floor(Math.random() * funIconImages.length)];
        currentIconX = Math.random() * (iconOverlayCanvas.width - 50);
        currentIconY = Math.random() * (iconOverlayCanvas.height - 50);
      }

      function drawIconOverlay() {
        iconOverlayCtx.clearRect(0, 0, iconOverlayCanvas.width, iconOverlayCanvas.height);
        iconOverlayCtx.drawImage(iconOverlayVideo, 0, 0, iconOverlayCanvas.width, iconOverlayCanvas.height);
        if (currentIcon) {
          let time = Date.now();
          let scale = 1 + 0.2 * Math.sin(time / 200);
          let size = 50 * scale;
          iconOverlayCtx.drawImage(currentIcon, currentIconX, currentIconY, size, size);
        }
      }

      function animateIconOverlay() {
        drawIconOverlay();
        iconOverlayAnimationFrame = requestAnimationFrame(animateIconOverlay);
      }

      function startIconOverlayLevelSystem() {
        iconOverlayLevelInterval = setInterval(() => {
          iconOverlayLevel++;
          document.getElementById("iconOverlayLevel").textContent = "Level: " + iconOverlayLevel;
          // Shorten icon update interval as level increases (improved: smoother transition)
          clearInterval(iconOverlayInterval);
          let newInterval = Math.max(500, 1500 - iconOverlayLevel * 50);
          iconOverlayInterval = setInterval(updateIconOverlay, newInterval);
        }, 10000);
      }

      async function startIconOverlay() {
        playClickSound();
        iconOverlayLoader.style.display = "block";
        try {
          iconOverlayStream = await navigator.mediaDevices.getUserMedia({ video: true });
          iconOverlayVideo.srcObject = iconOverlayStream;
          await iconOverlayVideo.play();
          iconOverlayCanvas.width = iconOverlayVideo.videoWidth;
          iconOverlayCanvas.height = iconOverlayVideo.videoHeight;
          iconOverlayLoader.style.display = "none";
          iconOverlayLevel = 1;
          document.getElementById("iconOverlayLevel").textContent = "Level: " + iconOverlayLevel;
          updateIconOverlay();
          iconOverlayInterval = setInterval(updateIconOverlay, 2000);
          startIconOverlayLevelSystem();
          animateIconOverlay();
        } catch (err) {
          console.error("Icon Overlay camera error:", err);
        }
      }

      function stopIconOverlay() {
        playClickSound();
        cancelAnimationFrame(iconOverlayAnimationFrame);
        clearInterval(iconOverlayInterval);
        clearInterval(iconOverlayLevelInterval);
        if (iconOverlayStream) {
          iconOverlayStream.getTracks().forEach(t => t.stop());
          iconOverlayStream = null;
        }
      }

      /* ----------------------------------------------------------------
         (J) EMOJI PARTY (Optimized with Face & Hands detection)
      ---------------------------------------------------------------- */
      let emojiPartyStream = null;
      let emojiPartyAnimationFrame = null;
      let emojiPartyFaceMesh = null;
      let emojiPartyHands = null;
      let emojiPartyCamera = null;
      let globalFaceLandmarksForParty = null;
      let emojiPartyFrameCount = 0;
      let emojiPartyLastTime = performance.now();

      const emojiPartyLoader = document.getElementById("emojiPartyLoader");
      const emojiPartyVideo = document.getElementById("emojiPartyVideo");
      const emojiPartyCanvas = document.getElementById("emojiPartyCanvas");
      const emojiPartyCtx = emojiPartyCanvas.getContext("2d");
      const emojiPartyFpsSpan = document.getElementById("emojiPartyFps");
      const emojiPartyCountSpan = document.getElementById("emojiPartyCount");

      // Task list for Emoji Party
      const emojiPartyTasks = [
        {
          descriptionEN: "Turn your head right",
          descriptionTR: "BaÅŸÄ±nÄ± saÄŸa Ã§evir",
          emoji: "ðŸ˜Ž",
          checkCondition: function() {
            if (!globalFaceLandmarksForParty) return false;
            let nose = globalFaceLandmarksForParty[1];
            if (!nose) return false;
            let sumX = 0;
            globalFaceLandmarksForParty.forEach(pt => sumX += pt.x);
            let centerX = sumX / globalFaceLandmarksForParty.length;
            return nose.x > (centerX + 0.015);
          }
        },
        {
          descriptionEN: "Blink your right eye",
          descriptionTR: "SaÄŸ gÃ¶zÃ¼nÃ¼ kÄ±rp",
          emoji: "ðŸ˜‰",
          checkCondition: function() {
            if (!globalFaceLandmarksForParty) return false;
            let leftPoint = globalFaceLandmarksForParty[33];
            let rightPoint = globalFaceLandmarksForParty[133];
            let topPoint = globalFaceLandmarksForParty[159];
            let bottomPoint = globalFaceLandmarksForParty[145];
            if (!leftPoint || !rightPoint || !topPoint || !bottomPoint) return false;
            let horizontalDist = rightPoint.x - leftPoint.x;
            let verticalDist = bottomPoint.y - topPoint.y;
            let ear = verticalDist / horizontalDist;
            return ear < 0.18;
          }
        },
        {
          descriptionEN: "Place your right hand on your head",
          descriptionTR: "SaÄŸ elini baÅŸÄ±na koy",
          emoji: "ðŸŽ©",
          checkCondition: function() {
            if (!globalFaceLandmarksForParty || !globalHandLandmarks) return false;
            let minY = 1;
            globalFaceLandmarksForParty.forEach(pt => {
              if (pt.y < minY) minY = pt.y;
            });
            let handPalm = globalHandLandmarks[0];
            if (!handPalm) return false;
            return handPalm.y < (minY + 0.05);
          }
        }
      ];
      let currentEmojiTaskIndex = 0;
      let emojiPartyTaskCompleted = false;
      let emojiCount = 0;

      async function initEmojiPartyFaceMesh() {
        emojiPartyFaceMesh = new FaceMesh({
          locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
        });
        emojiPartyFaceMesh.setOptions({
          maxNumFaces: 1,
          refineLandmarks: true,
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5,
        });
        emojiPartyFaceMesh.onResults((results) => {
          if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
            globalFaceLandmarksForParty = results.multiFaceLandmarks[0];
          } else {
            globalFaceLandmarksForParty = null;
          }
        });
      }

      async function initEmojiPartyHands() {
        emojiPartyHands = new Hands({
          locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`,
        });
        emojiPartyHands.setOptions({
          maxNumHands: 1,
          modelComplexity: 0,
          minDetectionConfidence: 0.3,
          minTrackingConfidence: 0.3,
        });
        emojiPartyHands.onResults((results) => {
          if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
            globalHandLandmarks = results.multiHandLandmarks[0];
          } else {
            globalHandLandmarks = null;
          }
        });
      }

      function drawEmojiPartyFrame() {
        emojiPartyCtx.clearRect(0, 0, emojiPartyCanvas.width, emojiPartyCanvas.height);
        emojiPartyCtx.drawImage(emojiPartyVideo, 0, 0, emojiPartyCanvas.width, emojiPartyCanvas.height);

        if (globalFaceLandmarksForParty) {
          globalFaceLandmarksForParty.forEach((pt) => {
            emojiPartyCtx.beginPath();
            emojiPartyCtx.arc(pt.x * emojiPartyCanvas.width, pt.y * emojiPartyCanvas.height, 2, 0, 2 * Math.PI);
            emojiPartyCtx.fillStyle = "red";
            emojiPartyCtx.fill();
          });
        }
        if (globalHandLandmarks) {
          globalHandLandmarks.forEach((pt) => {
            emojiPartyCtx.beginPath();
            emojiPartyCtx.arc(pt.x * emojiPartyCanvas.width, pt.y * emojiPartyCanvas.height, 4, 0, 2 * Math.PI);
            emojiPartyCtx.fillStyle = "green";
            emojiPartyCtx.fill();
          });
        }

        let currentTask = emojiPartyTasks[currentEmojiTaskIndex];
        let taskText = currentLang === "EN" ? currentTask.descriptionEN : currentTask.descriptionTR;
        emojiPartyCtx.fillStyle = "rgba(0,0,0,0.4)";
        emojiPartyCtx.fillRect(0, 0, emojiPartyCanvas.width, emojiPartyCanvas.height < 40 ? emojiPartyCanvas.height : 40);
        emojiPartyCtx.fillStyle = "white";
        emojiPartyCtx.font = "20px Arial";
        emojiPartyCtx.fillText(taskText, 10, 28);

        if (!emojiPartyTaskCompleted && currentTask.checkCondition()) {
          emojiPartyTaskCompleted = true;
          emojiCount++;
          emojiPartyCountSpan.textContent = "Emojis: " + emojiCount;
          let centerX = (emojiPartyCanvas.width / 2) - 40;
          let centerY = (emojiPartyCanvas.height / 2) + 30;
          // --- EMOJI BOYUTU ARTIRILDI: 80px ile gÃ¶sterilecek ---
          emojiPartyCtx.font = "80px Arial";
          emojiPartyCtx.fillText(currentTask.emoji, centerX, centerY);
          confetti({ particleCount: 100, spread: 60, origin: { y: 0.5 } });
          setTimeout(() => {
            currentEmojiTaskIndex = (currentEmojiTaskIndex + 1) % emojiPartyTasks.length;
            emojiPartyTaskCompleted = false;
          }, 2000);
        }
      }

      function emojiPartyLoop() {
        drawEmojiPartyFrame();
        emojiPartyFrameCount++;
        let now = performance.now();
        if (now - emojiPartyLastTime >= 1000) {
          emojiPartyFpsSpan.textContent = "FPS: " + emojiPartyFrameCount;
          emojiPartyFrameCount = 0;
          emojiPartyLastTime = now;
        }
        emojiPartyAnimationFrame = requestAnimationFrame(emojiPartyLoop);
      }

      async function startEmojiParty() {
        playClickSound();
        emojiPartyLoader.style.display = "block";
        if (!emojiPartyFaceMesh) await initEmojiPartyFaceMesh();
        if (!emojiPartyHands) await initEmojiPartyHands();
        try {
          emojiPartyStream = await navigator.mediaDevices.getUserMedia({ video: true });
          emojiPartyVideo.srcObject = emojiPartyStream;
          await emojiPartyVideo.play();
          emojiPartyCanvas.width = emojiPartyVideo.videoWidth;
          emojiPartyCanvas.height = emojiPartyVideo.videoHeight;
          emojiPartyLoader.style.display = "none";
          currentEmojiTaskIndex = 0;
          emojiPartyTaskCompleted = false;
          emojiCount = 0;
          emojiPartyCountSpan.textContent = "Emojis: " + emojiCount;
          emojiPartyCamera = new Camera(emojiPartyVideo, {
            onFrame: async () => {
              await emojiPartyFaceMesh.send({ image: emojiPartyVideo });
              await emojiPartyHands.send({ image: emojiPartyVideo });
            },
            width: emojiPartyVideo.videoWidth,
            height: emojiPartyVideo.videoHeight,
          });
          emojiPartyCamera.start();
          emojiPartyLoop();
        } catch (err) {
          console.error("Emoji Party camera error:", err);
        }
      }

      function stopEmojiParty() {
        playClickSound();
        cancelAnimationFrame(emojiPartyAnimationFrame);
        if (emojiPartyCamera) {
          emojiPartyCamera.stop();
          emojiPartyCamera = null;
        }
        if (emojiPartyStream) {
          emojiPartyStream.getTracks().forEach(t => t.stop());
          emojiPartyStream = null;
        }
      }

      /* ----------------------------------------------------------------
         (K) SOCIAL SHARING
      ---------------------------------------------------------------- */
      let shareModuleName = "";
      function openShareModal(moduleName) {
        playClickSound();
        shareModuleName = moduleName;
        const shareModal = new bootstrap.Modal(document.getElementById("shareModal"));
        shareModal.show();
      }

      function encodeURL(str) {
        return encodeURIComponent(str);
      }

      const shareMessage = `AI'Han Academy Rocks! I'm diving deep into AI and computer visionâ€”come and experience it for yourself!`;
      const shareURL = window.location.href;

      window.addEventListener("DOMContentLoaded", () => {
        document.getElementById("shareTwitterBtn").onclick = () => {
          const link = `https://twitter.com/intent/tweet?text=${encodeURL(shareMessage)}&url=${encodeURL(shareURL)}`;
          window.open(link, "_blank");
        };
        document.getElementById("shareFacebookBtn").onclick = () => {
          const link = `https://www.facebook.com/sharer.php?u=${encodeURL(shareURL)}&quote=${encodeURL(shareMessage)}`;
          window.open(link, "_blank");
        };
        document.getElementById("shareInstaBtn").onclick = () => {
          alert("Instagram web share isn't officially supported...");
          window.open("https://www.instagram.com/", "_blank");
        };
        document.getElementById("shareLinkedinBtn").onclick = () => {
          const link = `https://www.linkedin.com/sharing/share-offsite/?url=${encodeURL(shareURL)}&summary=${encodeURL(shareMessage)}`;
          window.open(link, "_blank");
        };

        doIntroSequence();
      });
    </script>
  </body>
</html>





